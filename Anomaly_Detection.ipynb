{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anomaly_Detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hschoi1/rich_latent/blob/master/Anomaly_Detection.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "JDZOiEuPZHZ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dependency imports\n",
        "from absl import flags\n",
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf\n",
        "import functools\n",
        "import itertools\n",
        "import os\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UA64SuJjZKg5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Flags\n",
        "# IMAGE_SHAPE = (28, 28, 1)\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:    \n",
        "       FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"learning_rate\", default=0.001, help=\"Initial learning rate.\")\n",
        "flags.DEFINE_integer(\n",
        "    \"max_steps\", default=2001, help=\"Number of training steps to run.\")\n",
        "flags.DEFINE_integer(\n",
        "    \"latent_size\",\n",
        "    default=16,\n",
        "    help=\"Number of dimensions in the latent code (z).\")\n",
        "flags.DEFINE_integer(\"base_depth\", default=32, help=\"Base depth for layers.\")\n",
        "flags.DEFINE_string(\n",
        "    \"activation\",\n",
        "    default=\"leaky_relu\",\n",
        "    help=\"Activation function for all hidden layers.\")\n",
        "flags.DEFINE_integer(\n",
        "    \"batch_size\",\n",
        "    default=32,\n",
        "    help=\"Batch size.\")\n",
        "flags.DEFINE_integer(\n",
        "    \"n_samples\", default=16, help=\"Number of samples to use in encoding.\")\n",
        "flags.DEFINE_bool(\n",
        "    \"use_NF\",\n",
        "    default = True,\n",
        "    help = \"If False, normalizing flows are not applied\")\n",
        "flags.DEFINE_integer(\n",
        "    \"mixture_components\",\n",
        "    default=100,\n",
        "    help=\"Number of mixture components to use in the prior. Each component is \"\n",
        "         \"a diagonal normal distribution. The parameters of the components are \"\n",
        "         \"intialized randomly, and then learned along with the rest of the \"\n",
        "         \"parameters. If `analytic_kl` is True, `mixture_components` must be \"\n",
        "         \"set to `1`.\")\n",
        "flags.DEFINE_integer(\"n_flows\", default=6, help=\"Number of Normalizing Flows\")\n",
        "flags.DEFINE_float(\"elbo_threshold\", default=5.0, help=\"anomaly threshold for whole elbo\")\n",
        "flags.DEFINE_bool(\n",
        "    \"analytic_kl\",\n",
        "    default=False,\n",
        "    help=\"Whether or not to use the analytic version of the KL. When set to \"\n",
        "         \"False the E_{Z~q(Z|X)}[log p(Z)p(X|Z) - log q(Z|X)] form of the ELBO \"\n",
        "         \"will be used. Otherwise the -KL(q(Z|X) || p(Z)) + \"\n",
        "         \"E_{Z~q(Z|X)}[log p(X|Z)] form will be used. If analytic_kl is True, \"\n",
        "         \"then you must also specify `mixture_components=1`.\")\n",
        "flags.DEFINE_string(\n",
        "    \"data_dir\",\n",
        "    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"), \"vae/data\"),\n",
        "    help=\"Directory where data is stored (if using real data).\")\n",
        "flags.DEFINE_string(\n",
        "    \"model_dir\",\n",
        "    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"), \"vae/\"),\n",
        "    help=\"Directory to put the model's fit.\")\n",
        "flags.DEFINE_integer(\n",
        "    \"viz_steps\", default=500, help=\"Frequency at which to save visualizations.\")\n",
        "\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"delete_existing\",\n",
        "    default=False,\n",
        "    help=\"If true, deletes existing `model_dir` directory.\")\n",
        "\n",
        "\n",
        "FLAGS = flags.FLAGS\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggeht5DKZQ6a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "ROOT_PATH = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
        "FILE_TEMPLATE = \"binarized_mnist_{split}.amat\"\n",
        "\n",
        "def download(directory, filename):\n",
        "  \"\"\"Download a file.\"\"\"\n",
        "  filepath = os.path.join(directory, filename)\n",
        "  if tf.gfile.Exists(filepath):\n",
        "    return filepath\n",
        "  if not tf.gfile.Exists(directory):\n",
        "    tf.gfile.MakeDirs(directory)\n",
        "  url = os.path.join(ROOT_PATH, filename)\n",
        "  print(\"Downloading %s to %s\" % (url, filepath))\n",
        "  urllib.request.urlretrieve(url, filepath)\n",
        "  return filepath\n",
        "\n",
        "\n",
        "def static_mnist_dataset(directory, split_name):\n",
        "  \"\"\"Return binary static MNIST tf.data.Dataset.\"\"\"\n",
        "  amat_file = download(directory, FILE_TEMPLATE.format(split=split_name))\n",
        "  dataset = tf.data.TextLineDataset(amat_file)\n",
        "  str_to_arr = lambda string: np.array([char == \"1\" for char in string.split()])\n",
        "\n",
        "  def _parser(s):\n",
        "    booltensor = tf.py_func(str_to_arr, [s], tf.bool)\n",
        "    reshaped = tf.reshape(booltensor, [28, 28, 1])\n",
        "    return tf.to_float(reshaped), tf.constant(0, tf.int32)\n",
        "\n",
        "  return dataset.map(_parser)\n",
        "\n",
        "def build_fake_input_fns(batch_size, eval_repeat=1, image_shape=(28, 28, 1)):\n",
        "  \"\"\"Build fake MNIST-style data for unit testing.\"\"\"\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(\n",
        "      np.random.rand(batch_size, *image_shape).astype(\"float32\")).map(\n",
        "          lambda row: (row, 0)).batch(batch_size)\n",
        "\n",
        "  train_input_fn = lambda: dataset.repeat().make_one_shot_iterator().get_next()\n",
        "  eval_input_fn = lambda: dataset.repeat(eval_repeat).make_one_shot_iterator().get_next()\n",
        "  return train_input_fn, eval_input_fn\n",
        "\n",
        "\n",
        "def build_mnist_input_fns(data_dir, batch_size):\n",
        "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
        "\n",
        "  # Build an iterator over training batches.\n",
        "  training_dataset = static_mnist_dataset(data_dir, \"train\")\n",
        "  training_dataset = training_dataset.shuffle(50000).repeat().batch(batch_size)\n",
        "  train_input_fn = lambda: training_dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "  # Build an iterator over the heldout set.\n",
        "  eval_dataset = static_mnist_dataset(data_dir, \"valid\")\n",
        "  eval_dataset = eval_dataset.batch(batch_size)\n",
        "  eval_input_fn = lambda: eval_dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "  return train_input_fn, eval_input_fn\n",
        "\n",
        "def build_credit_dataset(batch_size):\n",
        "    df = pd.read_csv('drive/Colab Notebooks/creditcard.csv.zip',header=0, sep=',', quotechar='\"')\n",
        "    normal = df[df.Class == 0]\n",
        "    anormal = df[df.Class == 1]  \n",
        "\n",
        "\n",
        "    X_index = df.columns[1:-2]\n",
        "    normal_features = normal[X_index]\n",
        "    normal_time_amount = normal[['Time','Amount']]\n",
        "    normal_time_amount = (normal_time_amount- normal_time_amount.mean())/normal_time_amount.std()\n",
        "    normal_X = pd.concat([normal_features, normal_time_amount], axis=1)\n",
        "    normal_X = normal_X.as_matrix()\n",
        "    normal_X = normal_X.astype(np.float32)\n",
        "\n",
        "    # split into train/test splits\n",
        "    #choices = np.random.choice(normal_X.shape[0],1000)\n",
        "    n_anormal = anormal.shape[0]  # 492\n",
        "    np.random.shuffle(normal_X)\n",
        "    test_normal_X = normal_X[:n_anormal]\n",
        "    #test_normal_X = normal_X[choices]\n",
        "    train_normal_X = normal_X[n_anormal:]\n",
        "    \n",
        "    anormal_features = anormal[X_index]\n",
        "    anormal_time_amount = anormal[['Time','Amount']]\n",
        "    anormal_time_amount = (anormal_time_amount- anormal_time_amount.mean())/anormal_time_amount.std()\n",
        "    anormal_X = pd.concat([anormal_features, anormal_time_amount], axis=1)\n",
        "    anormal_X = anormal_X.as_matrix()\n",
        "    anormal_X = anormal_X.astype(np.float32)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((train_normal_X, np.zeros(train_normal_X.shape[0])))\n",
        "    dataset = dataset.shuffle(buffer_size=256)\n",
        "    dataset = dataset.repeat()      \n",
        "    dataset = dataset.batch(batch_size)   \n",
        "    \n",
        "    \n",
        "    eval_labels = np.concatenate([np.zeros(n_anormal), np.ones(n_anormal)])\n",
        "    eval_dataset=tf.data.Dataset.from_tensor_slices((np.concatenate([test_normal_X, anormal_X], axis=0), eval_labels))\n",
        "    #eval_labels = np.zeros(test_normal_X.shape[0])\n",
        "    #eval_dataset = tf.data.Dataset.from_tensor_slices((test_normal_X, eval_labels))\n",
        "    \n",
        "    \n",
        "    eval_dataset = eval_dataset.batch(batch_size)\n",
        "\n",
        "    train_input_fn = lambda: dataset.repeat().make_one_shot_iterator().get_next()\n",
        "    eval_input_fn = lambda: eval_dataset.make_one_shot_iterator().get_next()\n",
        "    return train_input_fn, eval_input_fn  \n",
        "  \n",
        "\n",
        "def build_keras_dataset(keras_dataset, batch_size, expand_last_dim=False):\n",
        "  (x_train, y_train), (x_test, y_test) = keras_dataset.load_data()\n",
        "  \n",
        "  if expand_last_dim:\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "    x_test = np.expand_dims(x_test, axis=-1)\n",
        "  \n",
        "  x_train = x_train.astype(np.float32)/255.\n",
        "  x_test = x_test.astype(np.float32)/255.\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "  eval_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "  \n",
        "  train_input_fn = lambda: train_dataset.repeat().make_one_shot_iterator().get_next()\n",
        "  eval_input_fn = lambda: eval_dataset.make_one_shot_iterator().get_next()\n",
        "  return train_input_fn, eval_input_fn\n",
        "\n",
        "def get_dataset(dataset_name, batch_size):\n",
        "  # unified dataset loading fn\n",
        "  if dataset_name == 'mnist':\n",
        "    return build_mnist_input_fns(FLAGS.data_dir, batch_size)\n",
        "  elif dataset_name == 'fashion_mnist':\n",
        "    return build_keras_dataset(tf.keras.datasets.fashion_mnist, batch_size)\n",
        "  elif dataset_name == 'noise':\n",
        "    return build_fake_input_fns(batch_size, eval_repeat=10)\n",
        "  elif dataset_name == 'cifar10':\n",
        "    return build_keras_dataset(tf.keras.datasets.cifar10, batch_size)\n",
        "  elif dataset_name =='credit_card':\n",
        "    return build_credit_dataset(batch_size)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCER84qdaWUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "  \n",
        "def init_once(x, name):\n",
        "  return tf.get_variable(name, initializer=x, trainable=False)\n",
        "\n",
        "def make_arflow(z_dist, latent_size, n_flows, hidden_size=(512, 512), invert=False):\n",
        "    chain = list(itertools.chain.from_iterable([\n",
        "        tfb.MaskedAutoregressiveFlow(\n",
        "            shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\n",
        "                hidden_size)),\n",
        "        tfb.Permute(init_once(np.random.permutation(latent_size), 'permute_%d' % i)),\n",
        "    ] for i in range(n_flows)))\n",
        "    return tfd.TransformedDistribution(distribution=z_dist, bijector=tfb.Chain(chain[:-1]))\n",
        "\n",
        "def make_NF_prior(latent_size, n_flows):\n",
        "    return make_arflow(tfd.MultivariateNormalDiag(\n",
        "        loc=tf.zeros([latent_size], dtype=tf.float32),\n",
        "        scale_diag = tf.ones([latent_size], dtype=tf.float32)),\n",
        "                       latent_size, n_flows)\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "def make_mixture_prior(latent_size, mixture_components):\n",
        "  \"\"\"Create the mixture of Gaussians prior distribution. Prior is learned.\n",
        "  Args:\n",
        "    latent_size: The dimensionality of the latent representation.\n",
        "    mixture_components: Number of elements of the mixture.\n",
        "  Returns:\n",
        "    random_prior: A `tf.distributions.Distribution` instance\n",
        "      representing the distribution over encodings in the absence of any\n",
        "      evidence.\n",
        "  \"\"\"\n",
        "  if mixture_components == 1:\n",
        "    # See the module docstring for why we don't learn the parameters here.\n",
        "    return tfd.MultivariateNormalDiag(\n",
        "        loc=tf.zeros([latent_size]),\n",
        "        scale_identity_multiplier=1.0)\n",
        "  else:\n",
        "    loc = tf.get_variable(name=\"loc\", shape=[mixture_components, latent_size])\n",
        "    raw_scale_diag = tf.get_variable(\n",
        "        name=\"raw_scale_diag\", shape=[mixture_components, latent_size])\n",
        "    mixture_logits = tf.get_variable(\n",
        "        name=\"mixture_logits\", shape=[mixture_components])\n",
        "\n",
        "    return tfd.MixtureSameFamily(\n",
        "        components_distribution=tfd.MultivariateNormalDiag(\n",
        "            loc=loc,\n",
        "            scale_diag=tf.nn.softplus(raw_scale_diag)),\n",
        "        mixture_distribution=tfd.Categorical(logits=mixture_logits),\n",
        "        name=\"prior\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4FpWzkcZWu4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Credit Card Model\n",
        "\n",
        "\n",
        "def _softplus_inverse(x):\n",
        "  \"\"\"Helper which computes the function inverse of `tf.nn.softplus`.\"\"\"\n",
        "  return tf.log(tf.expm1(x))\n",
        "\n",
        "def make_encoder(activation, latent_size, base_depth):\n",
        "  \"\"\"Create the encoder function.\n",
        "  Args:\n",
        "    activation: Activation function to use.\n",
        "    latent_size: The dimensionality of the encoding.\n",
        "    base_depth: The lowest depth for a layer.\n",
        "  Returns:\n",
        "    encoder: A `callable` mapping a `Tensor` of images to a\n",
        "      `tf.distributions.Distribution` instance over encodings.\n",
        "  \"\"\"\n",
        "\n",
        "  encoder_net = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(2 * latent_size, activation=None),\n",
        "  ])\n",
        "\n",
        "  def encoder(images):\n",
        "    net = encoder_net(images)\n",
        "    return tfd.MultivariateNormalDiag(\n",
        "        loc=net[..., :latent_size],\n",
        "        scale_diag=tf.nn.softplus(net[..., latent_size:] +\n",
        "                                  _softplus_inverse(1.0)),\n",
        "        name=\"code\")\n",
        "\n",
        "  return encoder\n",
        "\n",
        "\n",
        "def make_decoder(activation, latent_size, output_shape, base_depth):\n",
        "  \"\"\"Create the decoder function.\n",
        "  Args:\n",
        "    activation: Activation function to use.\n",
        "    latent_size: Dimensionality of the encoding.\n",
        "    output_shape: The output image shape.\n",
        "    base_depth: Smallest depth for a layer.\n",
        "  Returns:\n",
        "    decoder: A `callable` mapping a `Tensor` of encodings to a\n",
        "      `tf.distributions.Distribution` instance over images.\n",
        "  \"\"\"\n",
        "\n",
        "  decoder_net = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(30),\n",
        "  ])\n",
        "\n",
        "  def decoder(codes):\n",
        "    #pdb.set_trace()\n",
        "    original_shape = tf.shape(codes)\n",
        "    # Collapse the sample and batch dimension and convert to rank-4 tensor for\n",
        "    # use with a convolutional decoder network.\n",
        "    codes = tf.reshape(codes, (-1, 1, 1, latent_size))\n",
        "    logits = decoder_net(codes)\n",
        "    logits = tf.reshape(\n",
        "        logits, shape=tf.concat([original_shape[:-1], output_shape], axis=0))\n",
        "    return tfd.Independent(\n",
        "        tfd.Bernoulli(logits=logits),\n",
        "        reinterpreted_batch_ndims=len(output_shape),\n",
        "        name=\"image\")\n",
        "\n",
        "  return decoder\n",
        "\n",
        "\n",
        "\n",
        "def anomaly_model_fn(features, labels, mode, params, config):\n",
        "  \"\"\"Build the model function for use in an estimator.\n",
        "  Arguments:\n",
        "    features: The input features for the estimator.\n",
        "    labels: The labels, unused here.\n",
        "    mode: Signifies whether it is train or test or predict.\n",
        "    params: Some hyperparameters as a dictionary.\n",
        "    config: The RunConfig, unused here.\n",
        "  Returns:\n",
        "    EstimatorSpec: A tf.estimator.EstimatorSpec instance.\n",
        "  \"\"\"\n",
        "  #del labels, config\n",
        "  predictions = {}\n",
        "\n",
        "  if params[\"analytic_kl\"] and params[\"latent_size\"] != 1:\n",
        "    raise NotImplementedError(\n",
        "        \"Using `analytic_kl` is only supported when `mixture_components = 1` \"\n",
        "        \"since there's no closed form otherwise.\")\n",
        "    \n",
        "  encoder = make_encoder(params[\"activation\"],\n",
        "                         params[\"latent_size\"],\n",
        "                         params[\"base_depth\"])\n",
        "  \n",
        "  image_shape = features.get_shape().as_list()[1:]\n",
        "  decoder = make_decoder(params[\"activation\"],\n",
        "                         params[\"latent_size\"],\n",
        "                         image_shape,\n",
        "                         params[\"base_depth\"])\n",
        "  if params['use_NF']:\n",
        "    latent_prior = make_NF_prior(params[\"latent_size\"],params[\"n_flows\"])\n",
        "  else:\n",
        "    latent_prior = make_mixture_prior(params[\"latent_size\"],\n",
        "                                      params[\"mixture_components\"])\n",
        "\n",
        "  #pdb.set_trace()\n",
        "  \n",
        "  approx_posterior = encoder(features)\n",
        "  approx_posterior_sample = approx_posterior.sample(params[\"n_samples\"])\n",
        "  decoder_likelihood = decoder(approx_posterior_sample)\n",
        "\n",
        "\n",
        "  # `distortion` is just the negative log likelihood.\n",
        "  distortion = -decoder_likelihood.log_prob(features)\n",
        "  avg_distortion = tf.reduce_mean(distortion)\n",
        "  tf.summary.scalar(\"distortion\", avg_distortion)\n",
        "  \n",
        "  \n",
        "  if params[\"analytic_kl\"]:\n",
        "    raise ValueError('Not Completely Implemented!')\n",
        "    rate = tfd.kl_divergence(approx_posterior, latent_prior)\n",
        "  else:\n",
        "    rate = (approx_posterior.log_prob(approx_posterior_sample)\n",
        "            - latent_prior.log_prob(approx_posterior_sample))\n",
        "  avg_rate = tf.reduce_mean(rate)\n",
        "  tf.summary.scalar(\"rate\", avg_rate)\n",
        "\n",
        "  elbo_local = -(rate + distortion)\n",
        "\n",
        "  elbo = tf.reduce_mean(elbo_local)\n",
        "  loss = -elbo\n",
        "  tf.summary.scalar(\"elbo\", elbo)\n",
        "  \n",
        "  # negative log-likelihood of encoded inputs under likelihood model p(x|z)\n",
        "  # lower is better\n",
        "  predictions['distortion'] = distortion \n",
        "  predictions['rate'] = rate\n",
        "  predictions['elbo'] = elbo_local\n",
        "\n",
        "  importance_weighted_elbo = tf.reduce_mean(\n",
        "      tf.reduce_logsumexp(elbo_local, axis=0) -\n",
        "      tf.log(tf.to_float(params[\"n_samples\"])))\n",
        "  tf.summary.scalar(\"elbo/importance_weighted\", importance_weighted_elbo)\n",
        "\n",
        "\n",
        "  # Perform variational inference by minimizing the -ELBO.\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "  learning_rate = tf.train.cosine_decay(0.0001, global_step,\n",
        "                                        params[\"max_steps\"])\n",
        "  tf.summary.scalar(\"learning_rate\", learning_rate)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "  # estimator predictions for inference + visualization\n",
        "  predictions['approx_posterior_mean'] = approx_posterior.mean()\n",
        "  predictions['approx_posterior_stddev'] = approx_posterior.scale.diag\n",
        "  \n",
        "  # adversarial perturbation\n",
        "  grad, = tf.gradients(loss, features)\n",
        "  adversarial_example = features - .1 * tf.sign(grad) # optimize the gibberish to minimize loss.\n",
        "  predictions['adversarial_example'] = adversarial_example\n",
        "  #pdb.set_trace()\n",
        "  elbo_local.set_shape((FLAGS.n_samples, None))\n",
        "  elbo_local_mean = tf.reduce_mean(elbo_local,axis=0)\n",
        "  predictions['elbo_local_mean'] = elbo_local_mean\n",
        "  elbo_local_mean = tf.sigmoid(elbo_local_mean)\n",
        "  predictions['sigmoid'] = elbo_local_mean\n",
        "  mask = tf.greater(elbo_local_mean,params['elbo_threshold'])\n",
        "  predictions['class'] = tf.cast(mask, tf.int32) \n",
        "  #elbo_local_mean = tf.clip_by_value(elbo_local_mean, 0.0,1.0)\n",
        "  #thresholds = [0.0, 0.5, 1.0]\n",
        "  thresholds = np.arange(0.0,0.2,0.01).tolist()\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "       eval_metric_ops = {\n",
        "          \"elbo\": tf.metrics.mean(elbo),\n",
        "          \"elbo/importance_weighted\": tf.metrics.mean(importance_weighted_elbo),\n",
        "          \"rate\": tf.metrics.mean(avg_rate),\n",
        "          \"distortion\": tf.metrics.mean(avg_distortion),\n",
        "      },\n",
        "      predictions=predictions\n",
        "  )\n",
        "  \n",
        "  labels = tf.cast(labels, tf.int32)\n",
        "  \n",
        "  (metric_tensor1, update_op1) = tf.metrics.true_positives_at_thresholds(labels=labels, predictions=elbo_local_mean,thresholds =thresholds)\n",
        "  (metric_tensor2, update_op2) = tf.metrics.true_negatives_at_thresholds(labels=labels, predictions=elbo_local_mean,thresholds =thresholds)\n",
        "  (metric_tensor3, update_op3) = tf.metrics.false_positives_at_thresholds(labels=labels, predictions=elbo_local_mean, thresholds=thresholds)\n",
        "  (metric_tensor4, update_op4) = tf.metrics.false_negatives_at_thresholds(labels=labels, predictions=elbo_local_mean, thresholds=thresholds)\n",
        "\n",
        "  summary_hook = tf.train.SummarySaverHook(\n",
        "    save_steps=100,\n",
        "    output_dir=FLAGS.model_dir,\n",
        "    summary_op=tf.summary.merge_all())\n",
        "  \n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "      loss=loss,\n",
        "      train_op=train_op,\n",
        "       eval_metric_ops = {\n",
        "          \"elbo\": tf.metrics.mean(elbo),\n",
        "          \"elbo/importance_weighted\": tf.metrics.mean(importance_weighted_elbo),\n",
        "          \"rate\": tf.metrics.mean(avg_rate),\n",
        "          \"distortion\": tf.metrics.mean(avg_distortion),\n",
        "           \"TP\": (tf.cast(metric_tensor1, tf.float32), tf.cast(update_op1, tf.float32)),\n",
        "           \"TN\": (tf.cast(metric_tensor2, tf.float32), tf.cast(update_op2, tf.float32)),\n",
        "           \"FP\": (tf.cast(metric_tensor3, tf.float32), tf.cast(update_op3, tf.float32)),\n",
        "           \"FN\": (tf.cast(metric_tensor4, tf.float32), tf.cast(update_op4, tf.float32)),\n",
        "           \"auc\": tf.metrics.auc(labels=labels, predictions=elbo_local_mean),\n",
        "      },\n",
        "      predictions=predictions, training_hooks=[summary_hook]\n",
        "  )\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXPYBkD_ZZ9S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train Credit Card Model\n",
        "def statistics(labels, predictions):\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    for i in range(len(labels)):\n",
        "        if labels[i] == 1 and predictions[i] == 1:\n",
        "            TP += 1\n",
        "        elif labels[i] == 0 and predictions[i] == 1:\n",
        "            FP += 1\n",
        "        elif labels[i] == 0 and predictions[i] == 0:\n",
        "            TN += 1\n",
        "        elif labels[i] == 1 and predictions[i] == 0:\n",
        "            FN += 1\n",
        "    return TP,FP,TN,FN\n",
        "\n",
        "def print_stats(values, truth, thresholds, name):\n",
        "  for threshold in thresholds:\n",
        "    predictions = np.zeros(values.shape[0])\n",
        "    predictions[np.argwhere(values > threshold)] = 1\n",
        "    stats = statistics(truth, predictions)\n",
        "    print(name, str(threshold), \":  TP:\",str(stats[0]),\" FP:\",str(stats[1]),\" TN:\",str(stats[2]),\" FN:\",str(stats[3]))\n",
        "    \n",
        "  \n",
        "def main(argv):\n",
        "  ensemble_elbos = []\n",
        "  ensemble_posterior_means = []\n",
        "  ensemble_posterior_vars = []\n",
        "  M=5 # number of models in ensemble\n",
        "  for i in range(M):\n",
        "    params = FLAGS.flag_values_dict()\n",
        "    params[\"activation\"] = getattr(tf.nn, params[\"activation\"])\n",
        "\n",
        "    #FLAGS.model_dir = \"/usr/local/google/home/ejang/tmp/cifar10/vae%d\" % i\n",
        "    FLAGS.model_dir = \"drive/Colab Notebooks/creditcard/vae%d\" % i\n",
        "\n",
        "    if FLAGS.delete_existing and tf.gfile.Exists(FLAGS.model_dir):\n",
        "      tf.logging.warn(\"Deleting old log directory at {}\".format(FLAGS.model_dir))\n",
        "      tf.gfile.DeleteRecursively(FLAGS.model_dir)\n",
        "    tf.gfile.MakeDirs(FLAGS.model_dir)\n",
        "\n",
        "    train_input_fn, eval_input_fn = get_dataset('credit_card', FLAGS.batch_size)\n",
        "    \n",
        "    estimator = tf.estimator.Estimator(\n",
        "        anomaly_model_fn,\n",
        "        params=params,\n",
        "        config=tf.estimator.RunConfig(\n",
        "            model_dir=FLAGS.model_dir,\n",
        "            save_checkpoints_steps=FLAGS.viz_steps,\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    \n",
        "    for _ in range(FLAGS.max_steps // FLAGS.viz_steps):    \n",
        "      estimator.train(train_input_fn, steps=FLAGS.viz_steps)\n",
        "      eval_results = estimator.evaluate(eval_input_fn)\n",
        "      print(\"Evaluation_results:\\n\\t%s\\n\" % eval_results)\n",
        "    \n",
        "    \n",
        "    batch_results_ = list(estimator.predict(eval_input_fn,predict_keys=['elbo_local_mean', 'sigmoid', 'approx_posterior_mean', 'approx_posterior_stddev']))\n",
        "    elbo_local_mean = np.array([b['elbo_local_mean'] for b in batch_results_])\n",
        "    mean = np.array([b['approx_posterior_mean'] for b in batch_results_])\n",
        "    stddev = np.array([b['approx_posterior_stddev'] for b in batch_results_])\n",
        "    #sigmoid = np.array([b['sigmoid'] for b in batch_results_])\n",
        "    ensemble_elbos.append(elbo_local_mean)\n",
        "    ensemble_posterior_means.append(mean)\n",
        "    ensemble_posterior_vars.append(stddev**2)\n",
        "  \n",
        "  ensemble_elbos = np.array(ensemble_elbos)\n",
        "  ensemble_posterior_means = np.array(ensemble_posterior_means)\n",
        "  ensemble_posterior_vars = np.array(ensemble_posterior_vars)\n",
        "  \n",
        "  truth = np.concatenate([np.zeros(492), np.ones(492)])  \n",
        "  \n",
        "  \n",
        "  #single model - last one\n",
        "  \n",
        "  \n",
        "  f, axes = plt.subplots(1, 2, figsize=(5, 5))\n",
        "  #bins0 = np.linspace(0.0,8000,1000)\n",
        "  axes[0].hist(elbo_local_mean[:492])\n",
        "  axes[0].set_xlabel(\"single elbo from valid data\")\n",
        "  axes[1].hist(elbo_local_mean[492:])\n",
        "  axes[1].set_xlabel(\"single elbo from anomlies\")\n",
        "  \n",
        "  single_elbo = elbo_local_mean\n",
        "  single_elbo_clipped = np.clip(single_elbo,-150,150)\n",
        "  max_single_elbo = 300\n",
        "  single_elbo_clipped = single_elbo_clipped/max_single_elbo + 0.5\n",
        "  auroc_score = roc_auc_score(y_true=truth, y_score=single_elbo_clipped)  \n",
        "  ap_score = average_precision_score(y_true=truth, y_score=single_elbo_clipped)\n",
        "  print(\"using single elbo,  AUROC: \",str(auroc_score),\"  AP: \",str(ap_score))\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  # ensemble\n",
        "  #pdb.set_trace()\n",
        "  ensemble_elbo_mean = np.mean(ensemble_elbos, axis=0)\n",
        "  ensemble_elbo_mean_clipped = np.clip(ensemble_elbo_mean,-200,200)\n",
        "  max_ensemble_elbo_mean = 400\n",
        "  ensemble_elbo_mean_clipped = ensemble_elbo_mean_clipped/max_ensemble_elbo_mean + 0.5\n",
        "  \n",
        "  \n",
        "  ensemble_elbo_var = np.var(ensemble_elbos, axis=0)\n",
        "  ensemble_elbo_var_clipped = np.clip(ensemble_elbo_var,0,10000)\n",
        "  max_ensemble_elbo_var = 10000\n",
        "  ensemble_elbo_var_clipped = ensemble_elbo_var_clipped/max_ensemble_elbo_var\n",
        "  \n",
        "  ensemble_posterior_means_mean = np.mean(np.mean(ensemble_posterior_means,axis=2),axis=0)\n",
        "  ensemble_posterior_means_mean_clipped = np.clip(ensemble_posterior_means_mean, -1.0,1.0)\n",
        "  max_ensemble_posterior_means_mean = 2.0\n",
        "  ensemble_posterior_means_mean_clipped = ensemble_posterior_means_mean_clipped/max_ensemble_posterior_means_mean + 0.5\n",
        "  \n",
        "  ensemble_posterior_means_var = np.var(np.mean(ensemble_posterior_means,axis=2), axis=0)\n",
        "  ensemble_posterior_means_var_clipped = np.clip(ensemble_posterior_means_var,0.0,3.0)\n",
        "  max_ensemble_posterior_means_var = 3.0\n",
        "  ensemble_posterior_means_var_clipped =  ensemble_posterior_means_var_clipped/ max_ensemble_posterior_means_var\n",
        "    \n",
        "  ensemble_posterior_vars_mean =  np.mean(np.mean(ensemble_posterior_vars,axis=2),axis=0)\n",
        "  ensemble_posterior_vars_mean_clipped = np.clip(ensemble_posterior_vars_mean,0.0,6.0)\n",
        "  max_ensemble_posterior_vars_mean = 6.0\n",
        "  ensemble_posterior_vars_mean_clipped = ensemble_posterior_vars_mean_clipped/max_ensemble_posterior_vars_mean\n",
        "  \n",
        "  ensemble_posterior_vars_var =  np.var(np.mean(ensemble_posterior_vars,axis=2),axis=0)\n",
        "  ensemble_posterior_vars_var_clipped = np.clip(ensemble_posterior_vars_var,0,20.0)\n",
        "  max_ensemble_posterior_vars_var = 20.0\n",
        "  ensemble_posterior_vars_var_clipped = ensemble_posterior_vars_var_clipped/max_ensemble_posterior_vars_var\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  auroc_score = roc_auc_score(y_true=truth, y_score=ensemble_elbo_mean_clipped)  \n",
        "  ap_score = average_precision_score(y_true=truth, y_score=ensemble_elbo_mean_clipped)\n",
        "  print(\"using ensemble_elbo_mean,  AUROC: \",str(auroc_score),\"  AP: \",str(ap_score))\n",
        "  \n",
        "  #ensemble_elbo_var_thresholds = np.arange(0,500,25)\n",
        "  auroc_score = roc_auc_score(y_true=truth, y_score=ensemble_elbo_var_clipped)  \n",
        "  ap_score = average_precision_score(y_true=truth, y_score=ensemble_elbo_var_clipped)\n",
        "  print(\"using ensemble_elbo_var,  AUROC: \",str(auroc_score),\"  AP: \",str(ap_score))\n",
        "  \n",
        "  #print_stats(ensemble_elbo_var, truth, ensemble_elbo_var_thresholds, \"ensemble elbo var\")\n",
        "  \n",
        " \n",
        "  auroc_score = roc_auc_score(y_true=truth, y_score=ensemble_posterior_means_mean_clipped)  \n",
        "  ap_score = average_precision_score(y_true=truth, y_score=ensemble_posterior_means_mean_clipped)\n",
        "  print(\"using ensemble mean of posterior mean,  AUROC: \",str(auroc_score),\"  AP: \",str(ap_score))\n",
        "  \n",
        "  auroc_score = roc_auc_score(y_true=truth, y_score=ensemble_posterior_means_var_clipped)  \n",
        "  ap_score = average_precision_score(y_true=truth, y_score=ensemble_posterior_means_var_clipped)\n",
        "  print(\"using ensemble mean of posterior var,  AUROC: \",str(auroc_score),\"  AP: \",str(ap_score))\n",
        "  \n",
        "  auroc_score = roc_auc_score(y_true=truth, y_score=ensemble_posterior_vars_mean_clipped)  \n",
        "  ap_score = average_precision_score(y_true=truth, y_score=ensemble_posterior_vars_mean_clipped)\n",
        "  print(\"using ensemble var of posterior mean,  AUROC: \",str(auroc_score),\"  AP: \",str(ap_score))\n",
        "  \n",
        "  auroc_score = roc_auc_score(y_true=truth, y_score=ensemble_posterior_vars_var_clipped)  \n",
        "  ap_score = average_precision_score(y_true=truth, y_score=ensemble_posterior_vars_var_clipped)\n",
        "  print(\"using ensemble var of posterior var,  AUROC: \",str(auroc_score),\"  AP: \",str(ap_score))\n",
        "  \n",
        "  \n",
        "  f, axes = plt.subplots(6, 2, figsize=(20, 20))\n",
        "  \n",
        "  bins0 = np.linspace(-100.0,150.0,250)\n",
        "  axes[0,0].hist(ensemble_elbo_mean[:492])\n",
        "  axes[0,0].set_xlabel(\"ensemble mean of elbo from valid data\")\n",
        "  axes[0,1].hist(ensemble_elbo_mean[492:])\n",
        "  axes[0,1].set_xlabel(\"ensemble mean of elbo from anomlies\")\n",
        "  \n",
        "  bins1 = np.linspace(0.0,8000,1000)\n",
        "  axes[1,0].hist(ensemble_elbo_var[:492])\n",
        "  axes[1,0].set_xlabel(\"ensemble var of elbo from valid data\")\n",
        "  axes[1,1].hist(ensemble_elbo_var[492:])\n",
        "  axes[1,1].set_xlabel(\"ensemble var of elbo from anomlies\")\n",
        "  \n",
        "  bins2 = np.linspace(-1.0,1.0,100)\n",
        "  axes[2,0].hist(ensemble_posterior_means_mean[:492])\n",
        "  axes[2,0].set_xlabel(\"ensemble mean of  posterior mean from valid data\")\n",
        "  axes[2,1].hist(ensemble_posterior_means_mean[492:])\n",
        "  axes[2,1].set_xlabel(\"ensemble mean of  posterior mean from anomalies\")\n",
        "  bins3 = np.linspace(0,5,100)\n",
        "  axes[3,0].hist(ensemble_posterior_means_var[:492])\n",
        "  axes[3,0].set_xlabel(\"ensemble var of  posterior mean from valid data\")\n",
        "  axes[3,1].hist(ensemble_posterior_means_var[492:])\n",
        "  axes[3,1].set_xlabel(\"ensemble var of  posterior mean from anomalies\")\n",
        "  \n",
        "  bins4 = np.linspace(0,10,100)\n",
        "  axes[4,0].hist(ensemble_posterior_vars_mean[:492])\n",
        "  axes[4,0].set_xlabel(\"ensemble mean of  posterior var from valid data\")\n",
        "  axes[4,1].hist(ensemble_posterior_vars_mean[492:])\n",
        "  axes[4,1].set_xlabel(\"ensemble mean of  posterior var from anomalies\")\n",
        "  bins5 = np.linspace(0,100,1000)\n",
        "  axes[5,0].hist(ensemble_posterior_vars_var[:492])\n",
        "  axes[5,0].set_xlabel(\"ensemble var of  posterior var from valid data\")\n",
        "  axes[5,1].hist(ensemble_posterior_vars_var[492:])\n",
        "  axes[5,1].set_xlabel(\"ensemble var of  posterior var from anomalies\")\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  \n",
        "  ensemble_posterior_vars_mean_thresholds = np.arange(1, 2.2, 0.02)\n",
        "  print(\"using ensemble mean of posterior var\")\n",
        "  print_stats(ensemble_posterior_vars_mean, truth, ensemble_posterior_vars_mean_thresholds, \"ensemble mean of post var\")\n",
        "  \n",
        "  ensemble_posterior_means_var_thresholds = np.arange(0.3, 1.5, 0.1)\n",
        "  print(\"using ensemble var of posterior mean\")\n",
        "  print_stats(ensemble_posterior_means_var, truth, ensemble_posterior_means_var_thresholds, \"ensemble var of post mean\")\n",
        "  '''\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  f, axes = plt.subplots(4, 2, figsize=(20, 20))\n",
        "  bins1 = np.linspace(-2.0,1.0,50)\n",
        "  axes[0,0].hist(ensemble_posterior_means_mean[:492],bins1)\n",
        "  axes[0,0].set_xlabel(\"ensemble mean of  posterior mean from valid data\")\n",
        "  axes[0,1].hist(ensemble_posterior_means_mean[492:],bins1)\n",
        "  axes[0,1].set_xlabel(\"ensemble mean of  posterior mean from anomalies\")\n",
        "  bins2 = np.linspace(0,100,100)\n",
        "  axes[1,0].hist(ensemble_posterior_means_var[:492], bins2)\n",
        "  axes[1,0].set_xlabel(\"ensemble var of  posterior mean from valid data\")\n",
        "  axes[1,1].hist(ensemble_posterior_means_var[492:],bins2)\n",
        "  axes[1,1].set_xlabel(\"ensemble var of  posterior mean from anomalies\")\n",
        "  \n",
        "  bins3 = np.linspace(0,100,100)\n",
        "  axes[2,0].hist(ensemble_posterior_vars_mean[:492],bins3)\n",
        "  axes[2,0].set_xlabel(\"ensemble mean of  posterior var from valid data\")\n",
        "  axes[2,1].hist(ensemble_posterior_vars_mean[492:],bins3)\n",
        "  axes[2,1].set_xlabel(\"ensemble mean of  posterior var from anomalies\")\n",
        "  bins4 = np.linspace(0,6000,600)\n",
        "  axes[3,0].hist(ensemble_posterior_vars_var[:492],bins4)\n",
        "  axes[3,0].set_xlabel(\"ensemble var of  posterior var from valid data\")\n",
        "  axes[3,1].hist(ensemble_posterior_vars_var[492:],bins4)\n",
        "  axes[3,1].set_xlabel(\"ensemble var of  posterior var from anomalies\")\n",
        "  \n",
        "  '''\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T5ZsAGI6aA79",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _softplus_inverse(x):\n",
        "  \"\"\"Helper which computes the function inverse of `tf.nn.softplus`.\"\"\"\n",
        "  return tf.log(tf.expm1(x))\n",
        "\n",
        "def make_encoder(activation, latent_size, base_depth):\n",
        "  \"\"\"Create the encoder function.\n",
        "  Args:\n",
        "    activation: Activation function to use.\n",
        "    latent_size: The dimensionality of the encoding.\n",
        "    base_depth: The lowest depth for a layer.\n",
        "  Returns:\n",
        "    encoder: A `callable` mapping a `Tensor` of images to a\n",
        "      `tf.distributions.Distribution` instance over encodings.\n",
        "  \"\"\"\n",
        "  conv = functools.partial(\n",
        "      tf.keras.layers.Conv2D, padding=\"SAME\", activation=activation)\n",
        "\n",
        "  encoder_net = tf.keras.Sequential([\n",
        "      conv(base_depth, 5, 1),\n",
        "      conv(base_depth, 5, 2),\n",
        "      conv(2 * base_depth, 5, 1),\n",
        "      conv(2 * base_depth, 5, 2),\n",
        "      conv(4 * latent_size, 7, padding=\"VALID\"),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(2 * latent_size, activation=None),\n",
        "  ])\n",
        "\n",
        "  def encoder(images):\n",
        "    images = 2 * tf.cast(images, dtype=tf.float32) - 1\n",
        "    net = encoder_net(images)\n",
        "    return tfd.MultivariateNormalDiag(\n",
        "        loc=net[..., :latent_size],\n",
        "        scale_diag=tf.nn.softplus(net[..., latent_size:] +\n",
        "                                  _softplus_inverse(1.0)),\n",
        "        name=\"code\")\n",
        "\n",
        "  return encoder\n",
        "\n",
        "\n",
        "def make_decoder(activation, latent_size, output_shape, base_depth):\n",
        "  \"\"\"Create the decoder function.\n",
        "  Args:\n",
        "    activation: Activation function to use.\n",
        "    latent_size: Dimensionality of the encoding.\n",
        "    output_shape: The output image shape.\n",
        "    base_depth: Smallest depth for a layer.\n",
        "  Returns:\n",
        "    decoder: A `callable` mapping a `Tensor` of encodings to a\n",
        "      `tf.distributions.Distribution` instance over images.\n",
        "  \"\"\"\n",
        "  deconv = functools.partial(\n",
        "      tf.keras.layers.Conv2DTranspose, padding=\"SAME\", activation=activation)\n",
        "  conv = functools.partial(\n",
        "      tf.keras.layers.Conv2D, padding=\"SAME\", activation=activation)\n",
        "\n",
        "  # first filter has size 7 for mnist, 8 for cifar (32x32)\n",
        "  filter_width = 7 if output_shape[0] == 28 else 8\n",
        "  \n",
        "  decoder_net = tf.keras.Sequential([\n",
        "      deconv(2 * base_depth, filter_width, padding=\"VALID\"),\n",
        "      deconv(2 * base_depth, 5),\n",
        "      deconv(2 * base_depth, 5, 2),\n",
        "      deconv(base_depth, 5),\n",
        "      deconv(base_depth, 5, 2),\n",
        "      deconv(base_depth, 5),\n",
        "      conv(output_shape[-1], 5, activation=None),\n",
        "  ])\n",
        "\n",
        "  def decoder(codes):\n",
        "    original_shape = tf.shape(codes)\n",
        "    # Collapse the sample and batch dimension and convert to rank-4 tensor for\n",
        "    # use with a convolutional decoder network.\n",
        "    codes = tf.reshape(codes, (-1, 1, 1, latent_size))\n",
        "    logits = decoder_net(codes)\n",
        "    logits = tf.reshape(\n",
        "        logits, shape=tf.concat([original_shape[:-1], output_shape], axis=0))\n",
        "    return tfd.Independent(\n",
        "        tfd.Bernoulli(logits=logits),\n",
        "        reinterpreted_batch_ndims=len(output_shape),\n",
        "        name=\"image\")\n",
        "\n",
        "  return decoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode, params, config):\n",
        "  \"\"\"Build the model function for use in an estimator.\n",
        "  Arguments:\n",
        "    features: The input features for the estimator.\n",
        "    labels: The labels, unused here.\n",
        "    mode: Signifies whether it is train or test or predict.\n",
        "    params: Some hyperparameters as a dictionary.\n",
        "    config: The RunConfig, unused here.\n",
        "  Returns:\n",
        "    EstimatorSpec: A tf.estimator.EstimatorSpec instance.\n",
        "  \"\"\"\n",
        "  del labels, config\n",
        "  predictions = {}\n",
        "\n",
        "  if params[\"analytic_kl\"] and params[\"latent_size\"] != 1:\n",
        "    raise NotImplementedError(\n",
        "        \"Using `analytic_kl` is only supported when `mixture_components = 1` \"\n",
        "        \"since there's no closed form otherwise.\")\n",
        "    \n",
        "  encoder = make_encoder(params[\"activation\"],\n",
        "                         params[\"latent_size\"],\n",
        "                         params[\"base_depth\"])\n",
        "  \n",
        "  image_shape = features.get_shape().as_list()[1:]\n",
        "  decoder = make_decoder(params[\"activation\"],\n",
        "                         params[\"latent_size\"],\n",
        "                         image_shape,\n",
        "                         params[\"base_depth\"])\n",
        "  if params['use_NF']:\n",
        "    latent_prior = make_NF_prior(params[\"latent_size\"],params[\"n_flows\"])\n",
        "  else:\n",
        "    latent_prior = make_mixture_prior(params[\"latent_size\"],\n",
        "                                      params[\"mixture_components\"])\n",
        "\n",
        "  image_tile_summary(\"input\", tf.to_float(features), rows=1, cols=16)\n",
        "\n",
        "  approx_posterior = encoder(features)\n",
        "  approx_posterior_sample = approx_posterior.sample(params[\"n_samples\"])\n",
        "  decoder_likelihood = decoder(approx_posterior_sample)\n",
        "  image_tile_summary(\n",
        "      \"recon/sample\",\n",
        "      tf.to_float(decoder_likelihood.sample()[:3, :16]),\n",
        "      rows=3,\n",
        "      cols=16)\n",
        "  image_tile_summary(\n",
        "      \"recon/mean\",\n",
        "      decoder_likelihood.mean()[:3, :16],\n",
        "      rows=3,\n",
        "      cols=16)\n",
        "\n",
        "  # `distortion` is just the negative log likelihood.\n",
        "  distortion = -decoder_likelihood.log_prob(features)\n",
        "  avg_distortion = tf.reduce_mean(distortion)\n",
        "  tf.summary.scalar(\"distortion\", avg_distortion)\n",
        "  \n",
        "  \n",
        "  if params[\"analytic_kl\"]:\n",
        "    raise ValueError('Not Completely Implemented!')\n",
        "    rate = tfd.kl_divergence(approx_posterior, latent_prior)\n",
        "  else:\n",
        "    rate = (approx_posterior.log_prob(approx_posterior_sample)\n",
        "            - latent_prior.log_prob(approx_posterior_sample))\n",
        "  avg_rate = tf.reduce_mean(rate)\n",
        "  tf.summary.scalar(\"rate\", avg_rate)\n",
        "\n",
        "  elbo_local = -(rate + distortion)\n",
        "\n",
        "  elbo = tf.reduce_mean(elbo_local)\n",
        "  loss = -elbo\n",
        "  tf.summary.scalar(\"elbo\", elbo)\n",
        "  \n",
        "  # negative log-likelihood of encoded inputs under likelihood model p(x|z)\n",
        "  # lower is better\n",
        "  predictions['distortion'] = distortion \n",
        "  predictions['rate'] = rate\n",
        "  predictions['elbo'] = elbo_local\n",
        "\n",
        "  importance_weighted_elbo = tf.reduce_mean(\n",
        "      tf.reduce_logsumexp(elbo_local, axis=0) -\n",
        "      tf.log(tf.to_float(params[\"n_samples\"])))\n",
        "  tf.summary.scalar(\"elbo/importance_weighted\", importance_weighted_elbo)\n",
        "\n",
        "  # Decode samples from the prior for visualization.\n",
        "  random_image = decoder(latent_prior.sample(16))\n",
        "  image_tile_summary(\n",
        "      \"random/sample\", tf.to_float(random_image.sample()), rows=4, cols=4)\n",
        "  image_tile_summary(\"random/mean\", random_image.mean(), rows=4, cols=4)\n",
        "\n",
        "  # Perform variational inference by minimizing the -ELBO.\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "  learning_rate = tf.train.cosine_decay(params[\"learning_rate\"], global_step,\n",
        "                                        params[\"max_steps\"])\n",
        "  tf.summary.scalar(\"learning_rate\", learning_rate)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "  # estimator predictions for inference + visualization\n",
        "  predictions['approx_posterior_mean'] = approx_posterior.mean()\n",
        "  predictions['approx_posterior_stddev'] = approx_posterior.scale.diag\n",
        "  \n",
        "  # adversarial perturbation\n",
        "  grad, = tf.gradients(loss, features)\n",
        "  adversarial_example = features - .1 * tf.sign(grad) # optimize the gibberish to minimize loss.\n",
        "  predictions['adversarial_example'] = adversarial_example\n",
        "\n",
        "\n",
        "    \n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode,\n",
        "      loss=loss,\n",
        "      train_op=train_op,\n",
        "      eval_metric_ops = {\n",
        "          \"elbo\": tf.metrics.mean(elbo),\n",
        "          \"elbo/importance_weighted\": tf.metrics.mean(importance_weighted_elbo),\n",
        "          \"rate\": tf.metrics.mean(avg_rate),\n",
        "          \"distortion\": tf.metrics.mean(avg_distortion),},\n",
        "      predictions=predictions\n",
        "  )\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0x9EnYqKarqz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(argv):\n",
        "  \n",
        "  M=1 # number of models in ensemble\n",
        "  for i in range(M):\n",
        "    params = FLAGS.flag_values_dict()\n",
        "    params[\"activation\"] = getattr(tf.nn, params[\"activation\"])\n",
        "\n",
        "    #FLAGS.model_dir = \"/usr/local/google/home/ejang/tmp/cifar10/vae%d\" % i\n",
        "    FLAGS.model_dir = \"drive/Colab Notebooks/cifar10/vae%d\" % i\n",
        "\n",
        "    if FLAGS.delete_existing and tf.gfile.Exists(FLAGS.model_dir):\n",
        "      tf.logging.warn(\"Deleting old log directory at {}\".format(FLAGS.model_dir))\n",
        "      tf.gfile.DeleteRecursively(FLAGS.model_dir)\n",
        "    tf.gfile.MakeDirs(FLAGS.model_dir)\n",
        "\n",
        "    train_input_fn, eval_input_fn = get_dataset('cifar10', FLAGS.batch_size)\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "        model_fn,\n",
        "        params=params,\n",
        "        config=tf.estimator.RunConfig(\n",
        "            model_dir=FLAGS.model_dir,\n",
        "            save_checkpoints_steps=FLAGS.viz_steps,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    for _ in range(FLAGS.max_steps // FLAGS.viz_steps):\n",
        "      estimator.train(train_input_fn, steps=FLAGS.viz_steps)\n",
        "      eval_results = estimator.evaluate(eval_input_fn)\n",
        "      print(\"Evaluation_results:\\n\\t%s\\n\" % eval_results)\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  tf.app.run()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-qSTrbEvZiCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = FLAGS.flag_values_dict()\n",
        "params[\"activation\"] = getattr(tf.nn, params[\"activation\"])\n",
        "\n",
        "def eval_ensemble(input_fn):\n",
        "  model_results = []\n",
        "  adversarial_examples = []\n",
        "  for i in range(5):\n",
        "    #FLAGS.model_dir = \"/usr/local/google/home/ejang/tmp/fashion_mnist/vae%d\" % i\n",
        "    #FLAGS.model_dir =\"drive/Colab Notebooks/fashion_mnist/vae%d\" % i\n",
        "    FLAGS.model_dir =\"drive/Colab Notebooks/creditcard/vae%d\" % i\n",
        "    print('Evaluating eval samples for %s' % FLAGS.model_dir)\n",
        "    assert tf.train.latest_checkpoint(FLAGS.model_dir) is not None\n",
        "    \n",
        "    estimator = tf.estimator.Estimator(\n",
        "        #model_fn,\n",
        "        anomaly_model_fn,\n",
        "        params=params,\n",
        "        config=tf.estimator.RunConfig(\n",
        "            model_dir=FLAGS.model_dir,\n",
        "        )\n",
        "    )\n",
        "    batch_results_ = list(estimator.predict(\n",
        "        input_fn,\n",
        "        predict_keys=['rate', 'distortion', 'elbo', 'elbo_local_mean'],\n",
        "        yield_single_examples=False\n",
        "    ))\n",
        "    \n",
        "    elbos = np.concatenate([b['elbo'].T for b in batch_results_], axis=0)\n",
        "    elbos = np.sum(elbos, axis=1)\n",
        "\n",
        "    distortions = np.concatenate([b['distortion'].T for b in batch_results_], axis=0)\n",
        "    distortions = np.sum(distortions, axis=1)\n",
        "\n",
        "    rates = np.concatenate([b['rate'].T for b in batch_results_], axis=0)\n",
        "    rates = np.sum(rates, axis=1)\n",
        "   \n",
        "    \n",
        "    model_results.append((rates, distortions, elbos))\n",
        "    \n",
        "  return np.stack(model_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gra-2MOpZjyf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot Rate-Distortion Curve for true data (good model minimizes them both)\n",
        "\n",
        "def plot_rd(model_results):\n",
        "  f, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "  axes[0].scatter(model_results[0, 0, :], model_results[0, 1, :])\n",
        "  axes[0].set_xlabel('Rate')\n",
        "  axes[0].set_ylabel('Distortion')\n",
        "  axes[1].hist(model_results[0, 2, :])\n",
        "  axes[1].set_xlabel('ELBO')\n",
        "  \n",
        "def plot_rd2(model_results):\n",
        "  f, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "  axes[0].scatter(model_results[0, 0, :], model_results[0, 1, :])\n",
        "  axes[0].set_xlabel('Rate')\n",
        "  axes[0].set_ylabel('Distortion')\n",
        "  axes[0].set_xlim(right=600)\n",
        "  axes[0].set_ylim(bottom=-5000)\n",
        "  axes[1].hist(model_results[0, 2, :])\n",
        "  axes[1].set_xlabel('ELBO')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B2EQGIecZm6W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensemble Statistics\n",
        "def plot_ensemble_stats(model_results):\n",
        "  f, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "  for i, name in enumerate(['Rate', 'Distortion', 'ELBO']):\n",
        "    mean = np.mean(model_results[:, i, :], axis=0)\n",
        "    var = np.var(model_results[:, i, :], axis=0)\n",
        "    axes[i].scatter(mean, var)\n",
        "    axes[i].set_xlabel('Ensemble %s mean' % name)\n",
        "    axes[i].set_ylabel('Ensemble %s variance' % name)\n",
        "    \n",
        "def plot_ensemble_stats2(model_results):\n",
        "  f, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "  mean = np.mean(model_results[:, 0, :], axis=0)\n",
        "  var = np.var(model_results[:, 0, :], axis=0)\n",
        "  axes[0].scatter(mean, var)\n",
        "  axes[0].set_xlabel('Ensemble %s mean' % 'Rate')\n",
        "  axes[0].set_ylabel('Ensemble %s variance' % 'Rate')\n",
        "  axes[0].set_xlim(right=600)\n",
        "  axes[0].set_ylim(top=10000)\n",
        "  \n",
        "  mean = np.mean(model_results[:, 1, :], axis=0)\n",
        "  var = np.var(model_results[:, 1, :], axis=0)\n",
        "  axes[1].scatter(mean, var)\n",
        "  axes[1].set_xlabel('Ensemble %s mean' % 'Distortion')\n",
        "  axes[1].set_ylabel('Ensemble %s variance' % 'Distortion')\n",
        "  axes[1].set_xlim(left=-4000)\n",
        "  axes[1].set_ylim(top=200000)\n",
        "  \n",
        "  mean = np.mean(model_results[:, 2, :], axis=0)\n",
        "  var = np.var(model_results[:, 2, :], axis=0)\n",
        "  axes[2].scatter(mean, var)\n",
        "  axes[2].set_xlabel('Ensemble %s mean' % 'ELBO')\n",
        "  axes[2].set_ylabel('Ensemble %s variance' % 'ELBO')\n",
        "  axes[2].set_xlim(right=0)\n",
        "  axes[2].set_ylim(bottom=-100000,top=100000)\n",
        "  \n",
        "                     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmdY2x8_a9tH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_, eval_input_fn = get_data('fashion_mnist', FLAGS.batch_size)\n",
        "true_model_results_ = eval_ensemble(eval_input_fn)\n",
        "plot_rd(true_model_results_)\n",
        "plot_ensemble_stats(true_model_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vACGYGIla_l_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A quick test to make sure that we can do gradient descent computation is done correctly\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  x = tf.placeholder(tf.float32, (32, 28, 28, 1))\n",
        "  params = FLAGS.flag_values_dict()\n",
        "  params[\"activation\"] = getattr(tf.nn, params[\"activation\"])\n",
        "\n",
        "  espec = model_fn(x, 0, tf.estimator.ModeKeys.PREDICT, params, None)\n",
        "  adv_example = espec.predictions['adversarial_example']\n",
        "  saver = tf.train.Saver()\n",
        "  adversarial_batches = []\n",
        "  with tf.train.MonitoredSession() as sess:\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(\"/usr/local/google/home/ejang/tmp/fashion_mnist/vae%d\" % 0))\n",
        "    for b in range(10):\n",
        "      # A batch of adversarial examples\n",
        "      adv_example_ = np.random.normal(size=(32, 28, 28, 1))\n",
        "      # for i in range(3):\n",
        "      e_, adv_example_ = sess.run([espec.predictions['elbo'], adv_example], {x: adv_example_})\n",
        "      e_, _ = sess.run([espec.predictions['elbo'], adv_example], {x: adv_example_})\n",
        "      print('%d adversarial ELBO: %f' % (b, np.mean(e_)))\n",
        "      adversarial_batches.append(adv_example_)\n",
        "adversarial_inputs_ = np.concatenate(adversarial_batches, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ONrx1_SbBeS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# construct an input_fn from the adversarial examples\n",
        "def build_adversarial_input_fn(adversarial_inputs, batch_size):\n",
        "  dummy_label = 0\n",
        "  # just examine first model's adversarial inputs\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(adversarial_inputs).map(\n",
        "          lambda feature: (feature, dummy_label)).batch(batch_size)\n",
        "  eval_input_fn = lambda: dataset.make_one_shot_iterator().get_next()\n",
        "  return eval_input_fn\n",
        "\n",
        "adversarial_input_fn = build_adversarial_input_fn(adversarial_inputs_, FLAGS.batch_size)\n",
        "adv_batch_results_ = eval_ensemble(adversarial_input_fn)\n",
        "plot_rd(adv_batch_results_)\n",
        "plot_ensemble_stats(adv_batch_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}