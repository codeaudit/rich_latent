{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_visualizaitons.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hschoi1/rich_latent/blob/master/MNIST_visualizaitons.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Z5h2YR7PX6eQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "import pdb\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "tf.set_random_seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OUT1U4lWYC32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"./data/MNIST_data/\", one_hot=False)\n",
        "\n",
        "train_data = mnist.train.images.astype(float)\n",
        "train_labels = mnist.train.labels.astype(float)\n",
        "from sklearn.utils import shuffle\n",
        "train_data, train_labels = shuffle(train_data, train_labels, random_state=0)\n",
        "numbers = []\n",
        "for i in range(10):\n",
        "    numbers.append(train_data[train_labels==i, :])\n",
        "\n",
        "test_data = train_data[-100:]\n",
        "test_labels = train_labels[-100:]\n",
        "\n",
        "train_data = train_data[:-100]\n",
        "train_labels = train_labels[:-100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yY-yvCwWYIkS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ensemble_mean_var(ensemble_results):\n",
        "    M = len(ensemble_results)\n",
        "    final_mean = 0\n",
        "    final_var = 0\n",
        "    for i in range(M):\n",
        "        mean, var = ensemble_results[i]\n",
        "        final_mean += mean / M\n",
        "        final_var += (var + np.square(mean))/ M\n",
        "    final_var -= np.square(final_mean)\n",
        "    return final_mean, final_var\n",
        "\n",
        "\n",
        "# log p(x|zk) specified by the paper\n",
        "def loss_fct(mean, var, labels):\n",
        "    loss = 0\n",
        "    loss += tf.log(var) / 2\n",
        "    loss += tf.square(labels - mean) / (2 * var)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_batch(data_x, batch_size=100):\n",
        "    batch_n = len(data_x) // batch_size\n",
        "    for i in range(batch_n):\n",
        "        batch_x = data_x[i * batch_size:(i + 1) * batch_size]\n",
        "        yield batch_x\n",
        "        \n",
        "def mean_var(results):\n",
        "    results = tf.cast(results, dtype=tf.float32)\n",
        "    predictive_mean, predictive_variance = tf.nn.moments(results, axes=[0])\n",
        "    return predictive_mean, predictive_variance\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PyjXd2iwYOoy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "num_models = 5\n",
        "ensemble_results = []\n",
        "adv_ensemble_results = []\n",
        "adv_ensemble_results2 = []\n",
        "adv_latent_prob_results1 = []\n",
        "adv_latent_prob_results2 = []\n",
        "adv_elbo = []\n",
        "adv_elbo2 = []\n",
        "\n",
        "def make_encoder(x, z_size=2):  \n",
        "    x = tf.cast(x, dtype=tf.float32)\n",
        "    net = make_enc_nn(x, z_size * 2)\n",
        "    #return tfd.MultivariateNormalDiag(loc=net[..., :z_size], scale_diag=tf.nn.softplus(net[..., z_size:]))\n",
        "    return net[..., :z_size], tf.nn.softplus(net[..., z_size:])\n",
        "\n",
        "\n",
        "def make_decoder(z, x_shape=784):\n",
        "    net = make_dec_nn(z, x_shape)\n",
        "    #mu_net = net[...,:x_shape]\n",
        "    #sigma_net = net[...,x_shape:]\n",
        "    return tfd.Independent(tfd.Bernoulli(logits=net))\n",
        "\n",
        "def make_enc_nn(x, out_size, hidden_size=(128,128)):\n",
        "    net = x\n",
        "    for h in hidden_size:\n",
        "        net = tf.layers.dense(inputs=net, units=h, activation=tf.nn.relu)\n",
        "    return tf.layers.dense(inputs=net, units=out_size)\n",
        "\n",
        "\n",
        "def make_dec_nn(x, out_size, hidden_size=(256,256,256)):\n",
        "    net = x\n",
        "    for h in hidden_size:\n",
        "        net = tf.layers.dense(inputs=net, units=h, activation=tf.nn.relu)\n",
        "    return tf.layers.dense(inputs=net, units=out_size)\n",
        "\n",
        "\n",
        "\n",
        "def make_arflow(z_dist, n_flows=8, hidden_size=(512,) * 2, invert=False):\n",
        "    chain = list(itertools.chain.from_iterable([tfb.MaskedAutoregressiveFlow(\n",
        "        shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(hidden_size)),\n",
        "                                                   tfb.Permute(np.random.permutation(2)), ] for _ in range(n_flows)))\n",
        "    return tfd.TransformedDistribution(distribution=z_dist, bijector=tfb.Chain(chain[:-1]))\n",
        "\n",
        "def make_prior(z_size=2):\n",
        "    return make_arflow(tfd.MultivariateNormalDiag(loc=tf.zeros([z_size], dtype=tf.float32), scale_diag = tf.ones([z_size], dtype=tf.float32)))\n",
        "\n",
        "\n",
        "# replicate paper\n",
        "\n",
        "# check if it really works\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    input_layer = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
        "    output_layer = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
        "    \n",
        "    \n",
        "    latent = make_prior()\n",
        "\n",
        "    # sample z0\n",
        "    with tf.variable_scope(\"encoder\"):\n",
        "      enc_mean, enc_std = make_encoder(input_layer)\n",
        "    enc = tfd.MultivariateNormalDiag(loc=enc_mean, scale_diag=enc_std)\n",
        "    enc_samples = enc.sample()\n",
        "    \n",
        "    \n",
        "    posterior_samples = enc.sample(100)\n",
        "    samples = [latent.bijector.inverse(posterior_samples)]\n",
        "    samples.append(posterior_samples)\n",
        "    \n",
        "    '''\n",
        "    #my_samples = enc.sample(100)\n",
        "    latent_samples = latent.sample(100)\n",
        "    samples = [latent_samples]\n",
        "    \n",
        "    # construct IAF flows\n",
        "    \n",
        "    z_tmp = latent.bijector.inverse(latent_samples)\n",
        "    samples = [z_tmp]\n",
        "\n",
        "    names = [enc.name]\n",
        "    names = [latent.name]\n",
        "    # transform back to the prior\n",
        "#     for bijector in latent.bijector.bijectors:\n",
        "#         latent_samples = bijector.inverse(latent_samples)\n",
        "#         samples.append(latent_samples) \n",
        "        \n",
        "#     samples = samples[::-1]\n",
        "    for bijector in reversed(latent.bijector.bijectors):\n",
        "         z_tmp = bijector.forward(z_tmp)\n",
        "         samples.append(z_tmp)\n",
        "         names.append(bijector.name)\n",
        "    # import pdb; pdb.set_trace()\n",
        "    '''\n",
        "    # reconstruct input\n",
        "\n",
        "    with tf.variable_scope(\"decoder\"):\n",
        "      recons = make_decoder(enc_samples)\n",
        "\n",
        "    recons_mean = recons.mean()\n",
        "    recons_var = recons.variance()\n",
        "   \n",
        "  \n",
        "    # elbo2  is reconstruction loss\n",
        "    # elbo1+elbo3 is \"kl divergence\"\n",
        "    \n",
        "    \n",
        "    elbo1_list = -latent.log_prob(enc_samples)\n",
        "    elbo1 = tf.reduce_mean(elbo1_list)\n",
        "    elbo2_list = -recons.log_prob(output_layer)\n",
        "    elbo2 = tf.reduce_mean(elbo2_list)\n",
        "    elbo3_list = enc.log_prob(enc_samples)\n",
        "    elbo3 = tf.reduce_mean(elbo3_list)\n",
        "    elbo_list = elbo1_list+elbo2_list+elbo3_list\n",
        "    elbo = tf.reduce_mean(elbo_list)\n",
        "    \n",
        "    gradients = tf.gradients(elbo, input_layer)\n",
        "    adv_examples = input_layer + 0.3*tf.sign(gradients)[0]\n",
        "    \n",
        "    \n",
        "    random_noise = tfd.Uniform(low=0.0, high=1.0)\n",
        "    random_noise_samples = random_noise.sample(sample_shape=(100,784))\n",
        "    adv_examples2 = random_noise_samples - 0.3*tf.sign(gradients)[0]\n",
        "    \n",
        "    gaussian_random_noise = tfd.Normal(loc=0.0, scale=1.0)\n",
        "    gaussian_random_noise_samples = gaussian_random_noise.sample(sample_shape=(100,784))\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #adv_examples = input_layer \n",
        "    \n",
        "    with tf.variable_scope(\"encoder\", reuse=True):\n",
        "      adv_enc_mean, adv_enc_std = make_encoder(adv_examples)\n",
        "    with tf.variable_scope(\"encoder\", reuse=True):  \n",
        "      adv_enc_mean2, adv_enc_std2 = make_encoder(adv_examples2)\n",
        "    with tf.variable_scope(\"encoder\", reuse=True):\n",
        "      adv_enc_mean3, adv_enc_std3 = make_encoder(gaussian_random_noise_samples)\n",
        "      \n",
        "    adv_enc = tfd.MultivariateNormalDiag(loc=adv_enc_mean, scale_diag=adv_enc_std)\n",
        "    adv_enc2 = tfd.MultivariateNormalDiag(loc=adv_enc_mean2, scale_diag=adv_enc_std2)\n",
        "    adv_enc3 = tfd.MultivariateNormalDiag(loc=adv_enc_mean3, scale_diag=adv_enc_std3)\n",
        "    \n",
        "    adv_enc_samples = adv_enc.sample()\n",
        "    adv_enc_samples2 = adv_enc2.sample()\n",
        "    adv_enc_samples3 = adv_enc3.sample()\n",
        "    \n",
        "    adv_posterior_samples = adv_enc.sample(100)\n",
        "    adv_posterior_samples2 = adv_enc2.sample(100)\n",
        "    adv_posterior_samples3 = adv_enc3.sample(100)\n",
        "    \n",
        "    adv_samples = [latent.bijector.inverse(adv_posterior_samples)]\n",
        "    adv_samples2 = [latent.bijector.inverse(adv_posterior_samples2)]\n",
        "    adv_samples3 = [latent.bijector.inverse(adv_posterior_samples3)]\n",
        "\n",
        "    adv_samples.append(adv_posterior_samples)\n",
        "    adv_samples2.append(adv_posterior_samples2)\n",
        "    adv_samples3.append(adv_posterior_samples3)\n",
        "    \n",
        "    with tf.variable_scope(\"decoder\", reuse=True):\n",
        "      adv_recons = make_decoder(adv_enc_samples)\n",
        "    with tf.variable_scope(\"decoder\", reuse=True):\n",
        "      adv_recons2 = make_decoder(adv_enc_samples2)\n",
        "    with tf.variable_scope(\"decoder\", reuse=True):\n",
        "      adv_recons3 = make_decoder(adv_enc_samples3)  \n",
        "\n",
        "    adv_recons_mean = adv_recons.mean()\n",
        "    adv_recons_mean2 = adv_recons2.mean()\n",
        "    adv_recons_mean3 = adv_recons3.mean()\n",
        "    \n",
        "    adv_recons_var = adv_recons.variance()\n",
        "    adv_recons_var2 = adv_recons2.variance()\n",
        "    adv_recons_var3 = adv_recons3.variance()\n",
        "    \n",
        "    adv_elbo1_list = - latent.log_prob(adv_enc_samples)\n",
        "    adv_elbo1 = tf.reduce_mean(adv_elbo1_list)\n",
        "    adv_elbo2_list = - adv_recons.log_prob(output_layer)\n",
        "    adv_elbo2 = tf.reduce_mean(adv_elbo2_list)\n",
        "    adv_elbo3_list = adv_enc.log_prob(adv_enc_samples)\n",
        "    adv_elbo3 = tf.reduce_mean(adv_elbo3_list)\n",
        "    adv_elbo_list = adv_elbo1_list + adv_elbo2_list + adv_elbo3_list\n",
        "    adv_elbo = adv_elbo1 + adv_elbo2 + adv_elbo3\n",
        "    \n",
        "    adv_elbo1_list2 = - latent.log_prob(adv_enc_samples2)\n",
        "    adv_elbo12 = tf.reduce_mean(adv_elbo1_list2)\n",
        "    adv_elbo2_list2 = - adv_recons2.log_prob(output_layer)\n",
        "    adv_elbo22 = tf.reduce_mean(adv_elbo2_list2)\n",
        "    adv_elbo3_list2 = adv_enc2.log_prob(adv_enc_samples2)\n",
        "    adv_elbo32 = tf.reduce_mean(adv_elbo3_list2)\n",
        "    adv_elbo_list2 = adv_elbo1_list2 + adv_elbo2_list2 + adv_elbo3_list2\n",
        "    adv_elbo2 = adv_elbo12 + adv_elbo22 + adv_elbo32\n",
        "    \n",
        "    optimizer = tf.train.AdamOptimizer(0.00001)\n",
        "    train_op = optimizer.minimize(elbo)\n",
        "    #f_train_op = optimizer.minimize(elbo, var_list=flows_variables+decoder_variables)\n",
        "\n",
        "\n",
        "    tf.summary.scalar('loss', elbo)\n",
        "    summ_op = tf.summary.merge_all()\n",
        "    train_writer = tf.summary.FileWriter('./logs/train')\n",
        "    saver = tf.train.Saver()\n",
        "    init = tf.global_variables_initializer()\n",
        "    g.finalize()\n",
        "\n",
        "# train and visualize\n",
        "with tf.Session(graph=g) as sess:\n",
        "   \n",
        "    train_writer.add_graph(g)\n",
        "    \n",
        "    for model in range(num_models):\n",
        "        step = 0\n",
        "        losses = []\n",
        "        losses_1 = []\n",
        "        losses_2 = []\n",
        "        losses_3 = []\n",
        "        global_step = []\n",
        "        sess.run(init)\n",
        "        \n",
        "        for epoch in range(20):\n",
        "        \n",
        "            train_data = shuffle(train_data, random_state=0)\n",
        "            print(\"model\",str(model),\"epoch\",str(epoch))\n",
        "\n",
        "            for batch_x in get_batch(train_data, 100):\n",
        "                # train\n",
        "                feed_dict = {input_layer: batch_x, output_layer:batch_x}\n",
        "                #if step <= 5000:\n",
        "                fetch_dict = {'loss': elbo, 'elbo1': elbo1, 'elbo2': elbo2, 'elbo3': elbo3, 'train': train_op, 'summary': summ_op}\n",
        "                #else:\n",
        "                #    fetch_dict = {'loss': elbo, 'elbo1': elbo1, 'elbo2': elbo2, 'elbo3': elbo3, 'train': f_train_op, 'summary': summ_op}\n",
        "                result = sess.run(fetch_dict, feed_dict)\n",
        "                # collect loss\n",
        "                if step % 400 == 0:\n",
        "                    train_writer.add_summary(result['summary'], step)\n",
        "                    global_step.append(step)\n",
        "                    losses.append(result['loss'])\n",
        "                    losses_1.append(result['elbo1'])\n",
        "                    losses_2.append(result['elbo2'])\n",
        "                    losses_3.append(result['elbo3'])\n",
        "                    print(\"loss at\", step, \":\", result['loss'], \"        elbo1:\", result[\"elbo1\"], \"  elbo2:\", result[\"elbo2\"], \"  elbo3:\", result[\"elbo3\"])\n",
        "\n",
        "                step += 1      \n",
        "\n",
        "        # testing\n",
        "          \n",
        "        #f, arr = plt.subplots(1, 3, figsize=(5 * 2, 5))\n",
        "        numbers_results=[]\n",
        "        adv_numbers_results1=[]\n",
        "        adv_numbers_results2=[]\n",
        "        adv_numbers_results3=[]\n",
        "        feed_dict = {input_layer:test_data, output_layer:test_data}\n",
        "        #if step <= 3000:\n",
        "        fetch_dict = {'recons_mean':recons_mean, 'recons_var':recons_var,'samples':samples, \"enc_mean\":enc_mean, \"enc_std\":enc_std,\n",
        "                      'adv_recons_mean':adv_recons_mean, 'adv_recons_var':adv_recons_var,\n",
        "                      'elbo_list':elbo_list,'elbo1_list':elbo1_list,'elbo2_list':elbo2_list,'elbo3_list':elbo3_list,\n",
        "                     'adv_elbo_list':adv_elbo_list,'adv_elbo1_list':adv_elbo1_list,'adv_elbo2_list':adv_elbo2_list,'adv_elbo3_list':adv_elbo3_list,\n",
        "                     'adv_examples':adv_examples, 'adv_examples2':adv_examples2,'adv_examples3':gaussian_random_noise_samples, 'adv_recons_mean2':adv_recons_mean2, \n",
        "                      'adv_recons_var2':adv_recons_var2,'adv_recons_mean3':adv_recons_mean3,'adv_recons_var3':adv_recons_var3, 'adv_elbo1_list2':adv_elbo1_list2,\n",
        "                          'adv_elbo2_list2':adv_elbo2_list2,'adv_elbo3_list2':adv_elbo3_list2, 'adv_elbo_list2':adv_elbo_list2}\n",
        "\n",
        "        test_results = sess.run(fetch_dict, feed_dict)\n",
        " \n",
        "\n",
        "        \n",
        "\n",
        "        num_layer = 2\n",
        "\n",
        "        # run digits from training distribution for visualization of latent space\n",
        "        for num_array in numbers:\n",
        "            feed_dict = {input_layer: num_array[:100], output_layer:num_array[:100]}\n",
        "            fetch_dict = {'samples': samples, 'adv_samples':adv_samples, 'adv_samples2':adv_samples2,'adv_samples3':adv_samples3,\n",
        "                         'elbo_list':elbo_list, 'adv_elbo1_list':adv_elbo1_list, 'adv_elbo1_list2':adv_elbo1_list2}\n",
        "\n",
        "            results = sess.run(fetch_dict, feed_dict)\n",
        "\n",
        "            num_results = results[\"samples\"]\n",
        "            adv_num_results1 = results[\"adv_samples\"]\n",
        "            adv_latent_prob1 = results[\"adv_elbo1_list\"]\n",
        "            adv_num_results2 = results[\"adv_samples2\"]\n",
        "            adv_latent_prob2 = results[\"adv_elbo1_list2\"]\n",
        "            adv_num_results3 = results[\"adv_samples3\"]\n",
        "            \n",
        "\n",
        "            numbers_results.append(num_results)\n",
        "            adv_numbers_results1.append(adv_num_results1)\n",
        "            adv_numbers_results2.append(adv_num_results2)\n",
        "            adv_numbers_results3.append(adv_num_results3)\n",
        "            adv_latent_prob_results1.append(adv_latent_prob1)\n",
        "            adv_latent_prob_results2.append(adv_latent_prob2)\n",
        "\n",
        "\n",
        "        # visualize latent of normal examples with color     \n",
        "        f, arr = plt.subplots(1, num_layer, figsize=(5 * num_layer, 5))\n",
        "        colors = cm.rainbow(np.linspace(0, 1, 10))\n",
        "\n",
        "        for i in range(num_layer):\n",
        "            for num, c in zip(range(10), colors):\n",
        "                X1 = numbers_results[num][i]\n",
        "                arr[i].scatter(X1[:, :100, 0], X1[:, :100, 1], s=1, color=c, label=num, alpha=0.5)\n",
        "                arr[i].set_xlim(-15,15)\n",
        "                arr[i].set_ylim(-15,15)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        \n",
        "        # visualize latent of adversarial examples1 with color\n",
        "        f, arr = plt.subplots(1, num_layer, figsize=(5 * num_layer, 5))\n",
        "        colors = cm.rainbow(np.linspace(0, 1, 10))\n",
        "\n",
        "        for i in range(num_layer):\n",
        "            for num, c in zip(range(10), colors):\n",
        "                X1 = adv_numbers_results1[num][i]\n",
        "                arr[i].scatter(X1[:, :100, 0], X1[:, :100, 1], s=1, color=c, label=num, alpha=0.5)\n",
        "                arr[i].set_xlim(-40,20)\n",
        "                arr[i].set_ylim(-10,50)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # visualize latent of adversarial examples2 with color\n",
        "        f, arr = plt.subplots(1, num_layer, figsize=(5 * num_layer, 5))\n",
        "        colors = cm.rainbow(np.linspace(0, 1, 10))\n",
        "\n",
        "        for i in range(num_layer):\n",
        "            for num, c in zip(range(10), colors):\n",
        "                X1 = adv_numbers_results2[num][i]\n",
        "                arr[i].scatter(X1[:, :100, 0], X1[:, :100, 1], s=1, color=c, label=num, alpha=0.5)\n",
        "                arr[i].set_xlim(-10,10)\n",
        "                arr[i].set_ylim(-10,10)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "   \n",
        "  \n",
        "        \n",
        "        f, arr = plt.subplots(1, num_layer, figsize=(5 * num_layer, 5))\n",
        "        colors = cm.rainbow(np.linspace(0, 1, 10))\n",
        "\n",
        "        for i in range(num_layer):\n",
        "            for num, c in zip(range(10), colors):\n",
        "                X1 = adv_numbers_results3[num][i]\n",
        "                arr[i].scatter(X1[:, :100, 0], X1[:, :100, 1], s=1, color=c, label=num, alpha=0.5)\n",
        "                arr[i].set_xlim(-10,10)\n",
        "                arr[i].set_ylim(-10,10)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "        f, arr = plt.subplots(1,4, figsize=(5 * num_layer, 5))\n",
        "        arr[0].plot(losses[1:])\n",
        "        arr[1].plot(losses_1[1:])\n",
        "        arr[2].plot(losses_2[1:])\n",
        "        arr[3].plot(losses_3[1:])\n",
        "        plt.show()\n",
        "          \n",
        "        ensemble_results.append((test_results[\"recons_mean\"],test_results[\"recons_var\"]))\n",
        "        adv_ensemble_results.append((test_results[\"adv_recons_mean\"], test_results[\"adv_recons_var\"]))\n",
        "        adv_ensemble_results2.append((test_results[\"adv_recons_mean2\"], test_results[\"adv_recons_var2\"]))\n",
        "        \n",
        "        saver.save(sess, './logs/ckpt/model-%d.ckpt' % (epoch))\n",
        "\n",
        "        \n",
        "        \n",
        "    # last single model input reconstruction\n",
        "    f, arr = plt.subplots(4, 3, figsize=(15, 15), sharex='all', sharey='all')\n",
        "\n",
        "\n",
        "    #normal points\n",
        "\n",
        "    single_mean = test_results['recons_mean']\n",
        "    single_var = test_results['recons_var']\n",
        "    arr[0, 0].imshow(np.reshape(test_data[1],[28,28]))\n",
        "    arr[0, 1].imshow(np.reshape(single_mean[1],[28,28]))\n",
        "    arr[0, 2].imshow(np.ones((28,28))-np.reshape(single_var[1],[28,28]))\n",
        "\n",
        "    #adversarial\n",
        "    adv_single_mean = test_results['adv_recons_mean']\n",
        "    adv_single_var = test_results['adv_recons_var']\n",
        "    arr[1, 0].imshow(np.reshape(test_results['adv_examples'][1],[28,28]))\n",
        "    arr[1, 1].imshow(np.reshape(adv_single_mean[1],[28,28]))\n",
        "    arr[1, 2].imshow(np.ones((28,28))-np.reshape(adv_single_var[1],[28,28]))\n",
        "    \n",
        "    #adversarial2\n",
        "    adv_single_mean2 = test_results['adv_recons_mean2']\n",
        "    adv_single_var2 = test_results['adv_recons_var2']\n",
        "    arr[2, 0].imshow(np.reshape(test_results['adv_examples2'][1],[28,28]))\n",
        "    arr[2, 1].imshow(np.reshape(adv_single_mean2[1],[28,28]))\n",
        "    arr[2, 2].imshow(np.ones((28,28))-np.reshape(adv_single_var2[1],[28,28]))\n",
        "    \n",
        "    #adversarial3\n",
        "    adv_single_mean3 = test_results['adv_recons_mean3']\n",
        "    adv_single_var3 = test_results['adv_recons_var3']\n",
        "    arr[3, 0].imshow(np.reshape(test_results['adv_examples3'][1],[28,28]))\n",
        "    arr[3, 1].imshow(np.reshape(adv_single_mean3[1],[28,28]))\n",
        "    arr[3, 2].imshow(np.ones((28,28))-np.reshape(adv_single_var3[1],[28,28]))\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "    # ensemble model input reconstruction\n",
        "\n",
        "\n",
        "    f, arr = plt.subplots(3, 3, figsize=(15, 15), sharex='all', sharey='all')\n",
        "    ensemble_mean, ensemble_var = ensemble_mean_var(ensemble_results)\n",
        "    adv_ensemble_mean, adv_ensemble_var = ensemble_mean_var(adv_ensemble_results)\n",
        "    adv_ensemble_mean2, adv_ensemble_var2 = ensemble_mean_var(adv_ensemble_results2)\n",
        "\n",
        "    #normal points\n",
        "\n",
        "    arr[0, 0].imshow(np.reshape(test_data[1],[28,28]))\n",
        "    arr[0, 1].imshow(np.reshape(ensemble_mean[1],[28,28]))\n",
        "    arr[0, 2].imshow(np.ones((28,28))-np.reshape(ensemble_var[1],[28,28]))\n",
        "\n",
        "    #adversarial   \n",
        "\n",
        "    arr[1, 0].imshow(np.reshape(test_results['adv_examples'][1],[28,28]))\n",
        "    arr[1, 1].imshow(np.reshape(adv_ensemble_mean[1],[28,28]))\n",
        "    arr[1, 2].imshow(np.ones((28,28))-np.reshape(adv_ensemble_var[1],[28,28]))\n",
        "    \n",
        "    \n",
        "    #adversarial2\n",
        "    \n",
        "    arr[2, 0].imshow(np.reshape(test_results['adv_examples2'][1],[28,28]))\n",
        "    arr[2, 1].imshow(np.reshape(adv_ensemble_mean2[1],[28,28]))\n",
        "    arr[2, 2].imshow(np.ones((28,28))-np.reshape(adv_ensemble_var2[1],[28,28]))\n",
        "    \n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    f, arr = plt.subplots(4, 2, figsize=(15, 15))\n",
        "    \n",
        "    arr[0,0].hist(single_var)\n",
        "    arr[0,1].hist(ensemble_var)\n",
        "    \n",
        "    arr[1,0].hist(adv_single_var)\n",
        "    arr[1,1].hist(adv_ensemble_var)\n",
        "    \n",
        "    arr[2,0].hist(adv_single_var2)\n",
        "    arr[2,1].hist(adv_ensemble_var2)\n",
        "    \n",
        "    arr[3,0].hist(test_results['elbo_list'])\n",
        "    arr[3,1].hist(test_results['adv_elbo_list2'])\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}